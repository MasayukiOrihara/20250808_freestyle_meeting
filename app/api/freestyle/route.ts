import { PromptTemplate } from "@langchain/core/prompts";
import { LangChainAdapter } from "ai";
import { FakeListChatModel } from "@langchain/core/utils/testing";

import { formatMessage, getInfoUsingTools } from "@/lib/utils";
import {
  FREESTYLE_COMPANY_SUMMARY,
  FREESTYLE_PROMPT,
  START_MESSAGE,
  TAVILY_CLIENT,
  TAVILY_ERROR,
} from "@/lib/contents";
import {
  client,
  Haiku3_5_YN,
  OpenAi,
  strParser,
  transportSearch,
} from "@/lib/models";

export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];

    console.log(" --- \nğŸ¢ FS API");

    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†
    const currentUserMessage = messages[messages.length - 1].content;
    const formattedPreviousMessages = messages.slice(1).map(formatMessage);

    const queryMessage = "site:freestyles.jp/ " + currentUserMessage;

    // ãƒ•ãƒªãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«ã®è©±ã‹ã©ã†ã‹ã®åˆ¤æ–­
    let isFreestyle = false;
    if (!currentUserMessage.includes(START_MESSAGE)) {
      const judgeTemplate =
        "ä»¥ä¸‹ã®ä¼šç¤¾æ¦‚è¦ã«ã€æ¬¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ–‡ç« ã¯é–¢é€£ã—ã¦ã„ã¾ã™ã‹ï¼Ÿã€ŒYESã€ã¾ãŸã¯ã€ŒNOã€ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n[ä¼šç¤¾æ¦‚è¦: {summry}]\n[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ–‡ç« : {input}]\n\nå‡ºåŠ›: ";
      const summry = FREESTYLE_COMPANY_SUMMARY;
      const checkJudgeFreestyle = await PromptTemplate.fromTemplate(
        judgeTemplate
      )
        .pipe(Haiku3_5_YN)
        .pipe(strParser)
        .invoke({ input: currentUserMessage, summry: summry });

      console.log("ğŸ¢ ãƒ•ãƒªãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«ã®è©±: " + checkJudgeFreestyle);
      if (checkJudgeFreestyle.includes("YES")) {
        isFreestyle = true;
      }
    }

    if (isFreestyle) {
      // API ãƒã‚§ãƒƒã‚¯
      const tavily = process.env.TAVILY_API_KEY;
      if (!tavily) throw new Error(TAVILY_ERROR);

      /** MCPã‚µãƒ¼ãƒãƒ¼ */
      const transport = transportSearch({ apiKey: tavily });
      const tavilyClient = client({ mcpName: TAVILY_CLIENT });
      await tavilyClient.connect(transport);

      /** AI */
      const prompt = PromptTemplate.fromTemplate(FREESTYLE_PROMPT);
      const info = await getInfoUsingTools(tavilyClient, queryMessage);
      const stream = await prompt.pipe(OpenAi).stream({
        history: formattedPreviousMessages,
        user_message: currentUserMessage,
        info: info,
      });

      console.log("ğŸ¢ COMPLITE \n --- ");

      return LangChainAdapter.toDataStreamResponse(stream);
    } else {
      //  ãƒ•ã‚§ã‚¤ã‚¯ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãã®ã¾ã¾å¿œç­”ã‚’é€ä¿¡
      const fakeModel = new FakeListChatModel({
        responses: ["é–¢é€£æ€§ãªã—"],
      });
      const prompt = PromptTemplate.fromTemplate("TEMPLATE1");
      const stream = await prompt.pipe(fakeModel).stream({});

      console.log("ğŸ¢ COMPLITE (NO USE) \n --- ");

      return LangChainAdapter.toDataStreamResponse(stream);
    }
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}
