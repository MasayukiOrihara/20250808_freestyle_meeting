import { ChatAnthropic } from "@langchain/anthropic";
import { PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { LangChainAdapter, Message as VercelChatMessage } from "ai";
import { StringOutputParser } from "@langchain/core/output_parsers";

const ANTHROPIC_MODEL_3_5 = "claude-3-5-haiku-20241022";

// openAI
const openAi = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
  model: "gpt-4o",
  temperature: 0.8,
  cache: true,
  tags: ["reflect_whiteboard"],
});

// anthropic(haiku-3.5)(langchainçµŒç”±)
const haiku3_5 = new ChatAnthropic({
  model: ANTHROPIC_MODEL_3_5,
  apiKey: process.env.ANTHROPIC_API_KEY!,
  maxTokens: 128,
  temperature: 0,
  cache: true,
  tags: ["reflect_whiteboard"],
});
const stringParser = new StringOutputParser();

// è³ªå•å†…å®¹
const mentorQuestions = [
  "å…·ä½“çš„ã«ã©ã‚“ãªã“ã¨ãŒã‚ã£ãŸï¼Ÿ",
  "ã„ã¤ã‹ã‚‰ãã®å•é¡ŒãŒã‚ã‚‹ï¼Ÿ",
  "é–¢ã‚ã£ã¦ã„ã‚‹äººã¯èª°ï¼Ÿ",
  "ã©ã“ã§èµ·ããŸï¼Ÿ",
  "ãã®æ™‚ã©ã‚“ãªæ°—æŒã¡ã ã£ãŸï¼Ÿ",
  "ä»Šã¯ã©ã†æ„Ÿã˜ã¦ã‚‹ï¼Ÿ",
  "ä¸€ç•ªå¼•ã£ã‹ã‹ã£ã¦ã„ã‚‹ã“ã¨ã¯ä½•ï¼Ÿ",
  "ã©ã†ã—ãŸã„ã¨æ€ã£ã¦ã„ã‚‹ï¼Ÿ",
  "ä»–ã«ã©ã‚“ãªé¸æŠè‚¢ãŒã‚ã‚‹ã¨æ€ã†ï¼Ÿ",
  "ä»Šã™ãã§ããã†ãªã“ã¨ã¯ä½•ï¼Ÿ",
];

export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];

    const currentUserMessage = messages[messages.length - 1].content;
    const formatMessage = (message: VercelChatMessage) => {
      return `${message.role}: ${message.content}`;
    };
    const formattedPreviousMessages = messages.slice(1).map(formatMessage);
    let prompt = PromptTemplate.fromTemplate(
      "ã‚ãªãŸã¯é™½æ°—ãªAIã§ã™ã€‚userã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦æ¬¡ã®æ–‡ç« ã‚’ã‹ãã ã›ã‚‹ã‚ˆã†ãªã‚³ãƒ¡ãƒ³ãƒˆã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯140æ–‡å­—ç¨‹åº¦ã§ã™ã€‚\n\nCurrent conversation: ---\n{history}\n---\n\nuser: {user_message}\nassistant: "
    );

    // æ‚©ã¿ç›¸è«‡ã‹ã©ã†ã‹ã®åˆ¤æ–­
    if (!currentUserMessage.includes("ã“ã‚“ã«ã¡ã¯")) {
      const judgeTemplate =
        "{question}\n\nã“ã®æ–‡ç« ã¯ æ‚©ã¿ã‚„ä¸å®‰ã‹ã‚‰ãã¦ã„ã‚‹ç›¸è«‡ ã§ã™ã‹ï¼Ÿ\nã€ŒYESã€ã¾ãŸã¯ã€ŒNOã€ã®ã©ã¡ã‚‰ã‹ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚";
      const checkJudgeMentor = await PromptTemplate.fromTemplate(judgeTemplate)
        .pipe(haiku3_5)
        .pipe(stringParser)
        .invoke({ question: currentUserMessage });

      if (checkJudgeMentor.includes("YES")) {
        console.log("ğŸ’› æ‚©ã¿ç›¸è«‡: " + checkJudgeMentor);
        prompt = PromptTemplate.fromTemplate(
          "ã‚ãªãŸã¯é™½æ°—ãªãƒ¡ãƒ³ã‚¿ãƒ¼AIã§ã™ã€‚userã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦ã€æ–‡è„ˆã«æ²¿ã†ã‚ˆã†ã«ä»¥ä¸‹ã® Question List ã‹ã‚‰è³ªå•æ–‡ã‚’1ã¤é¸ã‚“ã§140æ–‡å­—ç¨‹åº¦ã§userã«è³ªå•ã—ã¦ãã ã•ã„ã€‚è³ªå•ã¯140æ–‡å­—ä»¥å†…ã§ã™ã€‚\n\nCurrent conversation: ---\n{history}\n---\n\nQuestion List: ---\n{question_list}\n---\n\nuser: {user_message}\nassistant: "
        );
      }
    }

    const stream = await prompt.pipe(openAi).stream({
      question_list: mentorQuestions,
      history: formattedPreviousMessages,
      user_message: currentUserMessage,
    });

    console.log(currentUserMessage);

    return LangChainAdapter.toDataStreamResponse(stream);
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}
