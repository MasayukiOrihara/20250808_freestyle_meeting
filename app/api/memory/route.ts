import { RemoveMessage, SystemMessage } from "@langchain/core/messages";
import {
  Annotation,
  MemorySaver,
  MessagesAnnotation,
  StateGraph,
} from "@langchain/langgraph";

import { OpenAi4_1Mini } from "@/lib/models";
import { formattedMessage } from "./utils";

// „Éó„É≠„É≥„Éó„Éà: Ëã±Ë™û„Å´„Åó„Å¶ÁØÄÁ¥Ñ„Åó„Å¶„Åø„Çã (Ê≥®) „ÇÇ„ÅóËã±Ë™û„ÅßÂõûÁ≠î„Åó„Å†„ÅôÁî®„Å™„ÇâÊàª„Åô
/* ÂéüÊñá `Conversation summary so far: ${summary}\n\n‰∏äË®ò„ÅÆÊñ∞„Åó„ÅÑ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíËÄÉÊÖÆ„Åó„Å¶Ë¶ÅÁ¥Ñ„ÇíÊã°Âºµ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ: ` */
const MEMORY_UPDATE_PROMPT =
  "Here is the conversation summary so far: {summary}\n\nBased on the new message above, expand this summary while retaining important intent, information, and conversational flow for long-term memory.";
/* ÂéüÊñá "‰∏äË®ò„ÅÆÂÖ•Âäõ„ÇíÈÅéÂéª„ÅÆ‰ºöË©±„ÅÆË®òÊÜ∂„Å®„Åó„Å¶‰øùÊåÅ„Åß„Åç„Çã„Çà„ÅÜ„Å´ÈáçË¶Å„Å™ÊÑèÂõ≥„ÇÑÊÉÖÂ†±„ÉªÊµÅ„Çå„Åå„Çè„Åã„Çã„Çà„ÅÜ„Å´Áü≠„ÅèË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ: " */
const MEMORY_SUMMARY_PROMPT =
  "Summarize the input above concisely to preserve its key intent, information, and conversational flow, so it can be stored as memory for future context.";

/** „É°„ÉÉ„Çª„Éº„Ç∏„ÇíÊåøÂÖ•„Åô„ÇãÂá¶ÁêÜ */
async function insertMessages(state: typeof GraphAnnotation.State) {
  const messages = state.messages;
  return { messages: messages };
}

/** Ë¶ÅÁ¥Ñ„Åó„Åü„É°„ÉÉ„Çª„Éº„Ç∏„ÇíËøΩÂä†„Åô„ÇãÂá¶ÁêÜ */
async function prepareMessages(state: typeof GraphAnnotation.State) {
  const summary = state.summary;
  // Ë¶ÅÁ¥Ñ„Çí„Ç∑„Çπ„ÉÜ„É†„É°„ÉÉ„Çª„Éº„Ç∏„Å®„Åó„Å¶ËøΩÂä†
  const systemMessage = `Previous conversation summary: ${summary}`;
  const messages = [new SystemMessage(systemMessage)];

  return { messages: messages };
}

/** ‰ºöË©±„ÇíË°å„ÅÜ„ÅãË¶ÅÁ¥Ñ„Åô„Çã„Åã„ÅÆÂà§Êñ≠Âá¶ÁêÜ */
async function shouldContenue(state: typeof GraphAnnotation.State) {
  const messages = state.messages;

  if (messages.length > 6) return "summarize";
  return "__end__";
}

/** ‰ºöË©±„ÅÆË¶ÅÁ¥ÑÂá¶ÁêÜ */
async function summarizeConversation(state: typeof GraphAnnotation.State) {
  const summary = state.summary;

  let summaryMessage;

  if (summary) {
    summaryMessage = MEMORY_UPDATE_PROMPT.replace("{summary}", summary);
  } else {
    summaryMessage = MEMORY_SUMMARY_PROMPT;
  }

  // Ë¶ÅÁ¥ÑÂá¶ÁêÜ
  const messages = [...state.messages, new SystemMessage(summaryMessage)];
  const response = await OpenAi4_1Mini.invoke(messages);

  // Ë¶ÅÁ¥Ñ„Åó„Åü„É°„ÉÉ„Çª„Éº„Ç∏Èô§Âéª
  const deleteMessages = messages
    .slice(0, -2)
    .map((m) => new RemoveMessage({ id: m.id! }));
  return { summary: response.content, messages: deleteMessages };
}

// „Ç¢„Éé„ÉÜ„Éº„Ç∑„Éß„É≥„ÅÆËøΩÂä†
const GraphAnnotation = Annotation.Root({
  summary: Annotation<string>(),
  ...MessagesAnnotation.spec,
});

// „Ç∞„É©„Éï
const workflow = new StateGraph(GraphAnnotation)
  // „Éé„Éº„ÉâËøΩÂä†
  .addNode("insert", insertMessages)
  .addNode("prepare", prepareMessages)
  .addNode("summarize", summarizeConversation)

  // „Ç®„ÉÉ„Ç∏ËøΩÂä†
  .addEdge("__start__", "insert")
  .addConditionalEdges("insert", shouldContenue)
  .addEdge("summarize", "prepare")
  .addEdge("prepare", "__end__");

// Ë®òÊÜ∂„ÅÆËøΩÂä†
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
const cacheIdList: string[] = [];

/**
 * ‰ºöË©±Â±•Ê≠¥Ë¶ÅÁ¥ÑAPI
 * @param req
 * @returns
 */
export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];
    const threadId = body.threadId ?? "memory-abc123";

    // 2Ë°åÂèñÂæó
    const len = messages.length;
    const previousMessage = messages.slice(Math.max(0, len - 2), len);

    // Â±•Ê≠¥Áî®„Ç≠„Éº
    const config = { configurable: { thread_id: threadId } };
    const results = await app.invoke({ messages: previousMessage }, config);
    // ‰ºöË©±Â±•Ê≠¥„ÇíË®òÈå≤„Åó„Åü id „Çí„Åü„ÇÅËæº„ÇÄ
    const haveNotId = cacheIdList.every((id) => id !== threadId);
    if (haveNotId) {
      cacheIdList.push(threadId);
    }

    // Â±•Ê≠¥„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÂä†Â∑•
    const conversation = formattedMessage(results.messages, threadId);

    console.log("üí≥ Ë®òÊÜ∂ ---");
    console.log(conversation);
    console.log(" --- ");

    return new Response(JSON.stringify(conversation), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}

/**
 * „Åô„Åπ„Å¶„ÅÆ‰ºöË©±Â±•Ê≠¥Ë¶ÅÁ¥ÑAPI
 * @returns
 */
export async function GET() {
  try {
    if (cacheIdList && cacheIdList.length > 0) {
      for (const cache of cacheIdList) {
        const config = { configurable: { thread_id: cache } };
        const savedState = await memory.get(config);

        console.log(savedState);
      }
    }
    return new Response(JSON.stringify("conversation"), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}
