import {
  BaseMessage,
  RemoveMessage,
  SystemMessage,
} from "@langchain/core/messages";
import {
  Annotation,
  MemorySaver,
  MessagesAnnotation,
  StateGraph,
} from "@langchain/langgraph";

import { OpenAi4_1Mini } from "@/lib/models";
import {
  local,
  MEMORY_SUMMARY_PROMPT,
  MEMORY_UPDATE_PROMPT,
} from "@/lib/contents";
import {
  getConversasionSearch,
  getConversasionSearchSummary,
  getMessagSearch,
  postConversasionGenerate,
  postConversasionMessages,
  postConversasionSaveSummary,
} from "@/lib/api";
import { formatContent, formatConversation } from "./utils";

// å®šæ•°
const SUMMARY_MAX_COUNT = 6;

/** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹å‡¦ç† */
async function insartMessage(state: typeof GraphAnnotation.State) {
  const formatted: string[] = [];
  let summary = state.summary;

  // conversation ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‚ç…§
  let conversationId: string = await getConversasionSearch(state.sessionId);
  if (!conversationId) {
    // ã‚‚ã—å–å¾—ã§ããªã‹ã£ãŸå ´åˆã€æ–°ãŸã«conversationã‚’ä½œæˆã™ã‚‹
    conversationId = await postConversasionGenerate(state.sessionId);
    return { formatted: formatted };
  }
  // DB ã«ä¼šè©±è¦ç´„ã®ç¢ºèª
  if (!summary) {
    const savedSummary = await getConversasionSearchSummary(conversationId);
    if (savedSummary) summary = savedSummary;
    console.log("å–å¾—ã—ãŸè¦ç´„: " + savedSummary);
  }

  // DB ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’xä»¶å–å¾—ã™ã‚‹
  const count = state.turn % SUMMARY_MAX_COUNT;
  const latestMessage: string = await getMessagSearch(conversationId, count);

  return {
    formatted: [...formatted, ...latestMessage],
    summary: summary,
    conversationId: conversationId,
  };
}

/** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ DB ã«ç™»éŒ²ã™ã‚‹å‡¦ç† */
async function storeMessage(state: typeof GraphAnnotation.State) {
  const messages = state.messages;
  const formatted: string[] = state.formatted;

  // messages ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
  const { roles, contents } = formatContent(messages, state.sessionId);
  const length = Math.min(roles.length, contents.length);
  for (let i = 0; i < length; i++) {
    await postConversasionMessages(state.conversationId, roles[i], contents[i]);
  }

  // åŠ å·¥å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹
  const formattedMessages: string[] = formatConversation(roles, contents);

  // ä½¿ã£ãŸ messages ã¯åˆæœŸåŒ–
  const deleteMessages = messages.map((m) => new RemoveMessage({ id: m.id! }));
  return {
    messages: deleteMessages,
    formatted: [...formatted, ...formattedMessages],
  };
}

/** ä¼šè©±ã‚’è¡Œã†ã‹è¦ç´„ã™ã‚‹ã‹ã®åˆ¤æ–­å‡¦ç† */
async function shouldContenue(state: typeof GraphAnnotation.State) {
  const should = (state.turn + 1) % SUMMARY_MAX_COUNT === 0;
  if (should) return "summarize";
  return "marge";
}

/** ä¼šè©±ã®è¦ç´„ç”Ÿæˆ */
async function summarizeConversation(state: typeof GraphAnnotation.State) {
  let summary = state.summary;

  // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
  let summaryMessage;
  if (summary) {
    summaryMessage = MEMORY_UPDATE_PROMPT.replace("{summary}", summary);
  } else {
    summaryMessage = MEMORY_SUMMARY_PROMPT;
  }

  // è¦ç´„å‡¦ç†
  const messages = [new SystemMessage(summaryMessage)];
  const { roles, contents } = formatContent(messages, state.sessionId);
  const conversation: string[] = formatConversation(roles, contents);

  const formatted = [...state.formatted, ...conversation];
  console.log("ğŸ¶ ai ã«ä½•ã®æ–‡ç« ãƒ„ãƒƒã‚³ã‚€ã‹ç¢ºèª" + formatted);
  const response = await OpenAi4_1Mini.invoke(formatted);
  console.log("ğŸ¶ è¦ç´„ç¢ºèª" + response.content);

  return { summary: response.content };
}

/** è¦ç´„ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹å‡¦ç† */
async function prepareSummary(state: typeof GraphAnnotation.State) {
  const summary = state.summary;

  // è¦ç´„ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
  const systemMessage = `Previous conversation summary: ${summary}`;
  const messages = [new SystemMessage(systemMessage)];

  // ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
  const { roles, contents } = formatContent(messages, state.sessionId);
  const conversation: string[] = formatConversation(roles, contents);

  // DBã«è¿½åŠ 
  const id = await getConversasionSearch(state.sessionId);
  await postConversasionSaveSummary(id, conversation.join(""));
}

/** è¦ç´„æ–‡ã¨ä¼šè©±æ–‡ã®åˆæˆ */
async function margeSummaryAndFormatted(state: typeof GraphAnnotation.State) {
  const summary = state.summary;
  const formatted = state.formatted;

  let conversation;
  if (summary) {
    conversation = [summary, ...formatted];
  } else {
    conversation = [...formatted];
  }

  console.log("è¦ç´„" + summary);
  console.log("æœ€çµ‚å‡ºåŠ›: " + conversation);

  return { formatted: conversation };
}

// ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è¿½åŠ 
const GraphAnnotation = Annotation.Root({
  formatted: Annotation<string[]>(),
  summary: Annotation<string>(),
  sessionId: Annotation<string>(),
  turn: Annotation<number>(),
  conversationId: Annotation<string>(),

  ...MessagesAnnotation.spec,
});

// ã‚°ãƒ©ãƒ•
const workflow = new StateGraph(GraphAnnotation)
  // ãƒãƒ¼ãƒ‰è¿½åŠ 
  .addNode("insart", insartMessage)
  .addNode("save", storeMessage)
  .addNode("prepare", prepareSummary)
  .addNode("summarize", summarizeConversation)
  .addNode("marge", margeSummaryAndFormatted)

  // ã‚¨ãƒƒã‚¸è¿½åŠ 
  .addEdge("__start__", "insart")
  .addEdge("insart", "save")
  .addConditionalEdges("save", shouldContenue)
  .addEdge("summarize", "prepare")
  .addEdge("prepare", "marge")
  .addEdge("marge", "__end__");

// è¨˜æ†¶ã®è¿½åŠ 
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
const cacheIdList: string[] = [];

/**
 * ä¼šè©±å±¥æ­´è¦ç´„API
 * @param req
 * @returns
 */
export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];
    const threadId = body.threadId ?? "memory-abc123";
    const turn = body.turn ?? 0;

    // 2è¡Œå–å¾—
    const len = messages.length;
    const previousMessage: BaseMessage[] = messages.slice(
      Math.max(0, len - 2),
      len
    );

    // å±¥æ­´ç”¨ã‚­ãƒ¼
    const config = { configurable: { thread_id: threadId } };
    const results = await app.invoke(
      { messages: previousMessage, sessionId: threadId, turn: turn },
      config
    );

    // ä¼šè©±å±¥æ­´ã‚’è¨˜éŒ²ã—ãŸ id ã‚’ãŸã‚è¾¼ã‚€
    const haveNotId = cacheIdList.every((id) => id !== threadId);
    if (haveNotId) {
      cacheIdList.push(threadId);
    }

    console.log(results.formatted);

    return new Response(JSON.stringify(results.formatted), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}

/**
 * ã™ã¹ã¦ã®ä¼šè©±å±¥æ­´è¦ç´„API
 * @returns
 */
export async function GET() {
  try {
    if (cacheIdList && cacheIdList.length > 0) {
      for (const cache of cacheIdList) {
        const config = { configurable: { thread_id: cache } };
        const savedState = await memory.get(config);

        console.log(savedState);
      }
    }
    return new Response(JSON.stringify("conversation"), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}
