import {
  BaseMessage,
  RemoveMessage,
  SystemMessage,
} from "@langchain/core/messages";
import {
  Annotation,
  MemorySaver,
  MessagesAnnotation,
  StateGraph,
} from "@langchain/langgraph";

import { OpenAi4_1Mini } from "@/lib/models";
import {
  local,
  MEMORY_SUMMARY_PROMPT,
  MEMORY_UPDATE_PROMPT,
} from "@/lib/contents";
import {
  postConversasionGenerate,
  postConversasionSave,
  postConversasionSearch,
} from "@/lib/api";
import { formatContent, formatConversation } from "./utils";
import { ConversationMemory, MessageMemory } from "@/lib/types";

// å®šæ•°
const SUMMARY_MAX_COUNT = 6;

/** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’DBã‹ã‚‰å–å¾—ã™ã‚‹å‡¦ç† */
async function loadConversation(state: typeof GraphAnnotation.State) {
  const formatted: string[] = [];
  let summary = state.summary;
  const count = state.turn % SUMMARY_MAX_COUNT;

  // conversation ãƒ‡ãƒ¼ã‚¿å–å¾—
  const conversation: ConversationMemory = await postConversasionSearch(
    state.sessionId,
    count
  );

  console.log("ğŸ¶ conversationId" + conversation);
  let conversationId;
  if (conversation) {
    conversationId = conversation.id;
  } else {
    // ã‚‚ã—å–å¾—ã§ããªã‹ã£ãŸå ´åˆã€æ–°ãŸã«conversationã‚’ä½œæˆã™ã‚‹
    conversationId = await postConversasionGenerate(state.sessionId);
    return { formatted: formatted, conversationId: conversationId };
  }

  // ä¼šè©±è¦ç´„ã®ç¢ºèª
  if (!summary) {
    const savedSummary = conversation.summary;
    if (savedSummary && savedSummary != "") summary = savedSummary;
  }

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’xä»¶å–å¾—ã™ã‚‹
  const latestMessages = conversation.messages
    .reverse()
    .map((msg) => `${msg.role}: ${msg.content}`);

  return {
    formatted: [...formatted, ...latestMessages],
    summary: summary,
    conversationId: conversationId,
  };
}

/** ä¼šè©±ã‚’è¡Œã†ã‹è¦ç´„ã™ã‚‹ã‹ã®åˆ¤æ–­å‡¦ç† */
async function shouldContenue(state: typeof GraphAnnotation.State) {
  const should = (state.turn + 1) % SUMMARY_MAX_COUNT === 0;
  if (should) return "summarize";
  return "store";
}

/** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ DB ã«ç™»éŒ²ã™ã‚‹å‡¦ç† */
async function storeConversation(state: typeof GraphAnnotation.State) {
  const messages = state.messages;
  const formatted: string[] = state.formatted;

  // messages ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
  const { roles, contents } = formatContent(messages, state.sessionId);
  const length = Math.min(roles.length, contents.length);

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä½œæˆ
  const arrMessage = [];
  let message: MessageMemory;
  for (let i = 0; i < length; i++) {
    message = { role: roles[i], content: contents[i] };
    arrMessage.push(message);
  }
  // Conversation ã®ä½œæˆ
  const conversation: ConversationMemory = {
    id: state.conversationId,
    summary: state.summary,
    messages: arrMessage,
  };

  await postConversasionSave(conversation);

  // åŠ å·¥å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹
  const formattedMessages: string[] = formatConversation(roles, contents);

  // ä½¿ã£ãŸ messages ã¯åˆæœŸåŒ–
  const deleteMessages = messages.map((m) => new RemoveMessage({ id: m.id! }));
  return {
    messages: deleteMessages,
    formatted: [...formatted, ...formattedMessages],
  };
}

/** ä¼šè©±ã®è¦ç´„ç”Ÿæˆ */
async function summarizeConversation(state: typeof GraphAnnotation.State) {
  let summary = state.summary;

  // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
  let summaryMessage;
  if (summary) {
    summaryMessage = MEMORY_UPDATE_PROMPT.replace("{summary}", summary);
  } else {
    summaryMessage = MEMORY_SUMMARY_PROMPT;
  }

  // è¦ç´„å‡¦ç†
  const messages = [new SystemMessage(summaryMessage)];
  const { roles, contents } = formatContent(messages, state.sessionId);
  const conversation: string[] = formatConversation(roles, contents);

  const formatted = [...state.formatted, ...conversation];
  const response = await OpenAi4_1Mini.invoke(formatted);

  return { summary: response.content };
}

/** è¦ç´„ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹å‡¦ç† */
async function prepareSummary(state: typeof GraphAnnotation.State) {
  const summary = state.summary;

  // è¦ç´„ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
  const systemMessage = `Previous conversation summary: ${summary}`;
  const messages = [new SystemMessage(systemMessage)];

  // ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
  const { roles, contents } = formatContent(messages, state.sessionId);
  const conversation = formatConversation(roles, contents);

  return { summary: conversation.join("") };
}

/** è¦ç´„æ–‡ã¨ä¼šè©±æ–‡ã®åˆæˆ */
async function margeSummaryAndFormatted(state: typeof GraphAnnotation.State) {
  const summary = state.summary;
  const formatted = state.formatted;

  let conversation;
  if (summary) {
    conversation = [summary, ...formatted];
  } else {
    conversation = [...formatted];
  }

  return { formatted: conversation };
}

// ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è¿½åŠ 
const GraphAnnotation = Annotation.Root({
  formatted: Annotation<string[]>(),
  summary: Annotation<string>(),
  sessionId: Annotation<string>(),
  turn: Annotation<number>(),
  conversationId: Annotation<string>(),

  ...MessagesAnnotation.spec,
});

// ã‚°ãƒ©ãƒ•
const workflow = new StateGraph(GraphAnnotation)
  // ãƒãƒ¼ãƒ‰è¿½åŠ 
  .addNode("load", loadConversation)
  .addNode("store", storeConversation)
  .addNode("prepare", prepareSummary)
  .addNode("summarize", summarizeConversation)
  .addNode("marge", margeSummaryAndFormatted)

  // ã‚¨ãƒƒã‚¸è¿½åŠ 
  .addEdge("__start__", "load")
  .addConditionalEdges("load", shouldContenue)
  .addEdge("summarize", "prepare")
  .addEdge("prepare", "store")
  .addEdge("store", "marge")
  .addEdge("marge", "__end__");

// è¨˜æ†¶ã®è¿½åŠ 
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
const cacheIdList: string[] = [];

/**
 * ä¼šè©±å±¥æ­´è¦ç´„API
 * @param req
 * @returns
 */
export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];
    const threadId = body.threadId ?? "memory-abc123";
    const turn = body.turn ?? 0;

    // 2è¡Œå–å¾—
    const len = messages.length;
    const previousMessage: BaseMessage[] = messages.slice(
      Math.max(0, len - 2),
      len
    );

    // å±¥æ­´ç”¨ã‚­ãƒ¼
    const config = { configurable: { thread_id: threadId } };
    const results = await app.invoke(
      { messages: previousMessage, sessionId: threadId, turn: turn },
      config
    );

    return new Response(JSON.stringify(results.formatted), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}
